{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ch8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ceXcxnFyONlJ"
      },
      "source": [
        "# Automated Text Generation\n",
        "\n",
        "Feedforward neural nets are generally great for classification and regression problems. CNNs are great for complex image classification. But, activations for feedforward and CNNs flow only in one direction, from the input layers to the output layer. Since signals flow in only one direction, feedforward and convolutional nets are not ideal if patterns in data change over time. So, we need a different network architecture to work with data impacted by time.\n",
        "\n",
        "A **recurrent neural network** (RNN) looks a lot like a feedforward neural network, but it also has connections pointing backward. That is, output from one layer can act as input back into another layer earlier in the network. The capability of one layer to inform another layer earlier in the network means that a RNN has a built-in feedback loop mechanism that allows it to act as a forecasting engine. So, RNNs are great as forecasters because they naturally work well as data changes with time.\n",
        "\n",
        "A RNN remembers the past and its decisions are influenced by what it has learned from the past. Feedforward and convolutional networks only remember what they learn during training. For example, a feedforward image classifier learns what an image looks like during training and then uses that knowledge to classify other images in production. While RNNs learn similarly during training, they also remember what they learned so they can make good decisions as data changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XrE3vQtJaXho"
      },
      "source": [
        "# Natural Language Processing\n",
        "\n",
        "A fascinating advancement in maching learning is the ability to teach a machine how to understand human communication. The area of machine learning that concentrates on understanding how humans communicate is natural language processing. Formally, **natural language processing** (NLP) is a field in machine learning concentrating on the ability of a computer to understand, analyze, manipulate, and potentially generate human language. RNNs are a very important variant of neural networks used heavily for NLP.\n",
        "\n",
        "RNNs are great for NLP because their standard input is a word instead of the entire sample taken as standard input by sequential nets like feedforward and convolutional networks. So, RNNs have the flexibility to work with varying lengths of sentences, which cannot be achieved by sequential neural networks because of thier fixed structure. RNNs can also share features learned across different positions of text because of their flexible structure.\n",
        "\n",
        "The feedback loop ability of a RNN allows it to parse each word of a sentence and run an activation on it. The activation value from the word can then be fed back to the layer that is parsing the sentence. So, the activation value informs the sentence of what was learned from each word! And, the cycle continues for each word until the network understands the sentence. In machine learning speak, a RNN treats each word of a sentence as a separate input occurring at a particular time 't' and uses the activation value for this input 't-1' as feedback to the orignal sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czUM6uWXNuqh",
        "colab_type": "text"
      },
      "source": [
        "Enable the GPU (if not already enabled):\n",
        "1.\tclick **Runtime** in the top left menu\n",
        "2.\tclick **Change runtime type** from the drop-down menu\n",
        "3.\tchoose **GPU** from the *Hardware accelerator* drop-down menu\n",
        "4.\tclick **SAVE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpf4wFI22n5e",
        "colab_type": "text"
      },
      "source": [
        "Test is GPU is active:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd501k9RNwpG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f6cefb2d-ce7a-4642-d793-77579008936c"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# display tf version and test if GPU is active\n",
        "\n",
        "tf.__version__, tf.test.gpu_device_name()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('2.3.0', '/device:GPU:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX0IlUOf2qfy",
        "colab_type": "text"
      },
      "source": [
        "Import the tensorflow library. If '/device:GPU:0' is displayed, the GPU is active. If '..' is displayed, the regular CPU is active."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dHQz25F3SIDt"
      },
      "source": [
        "# Generating Text with a Character-Level RNN Model\n",
        "\n",
        "As noted, RNNs are commonly used for natural language tasks. Typically, we can model natural language tasks by character or word. We begin by building a **character-level** model that generates text. In the next chapter, we build a word-level model that predicts sentiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InNmICLdWQTV",
        "colab_type": "text"
      },
      "source": [
        "## The Text File\n",
        "\n",
        "We are going to work with **A Tale of Two Cities** by Charles Dickens. For convenience, we’ve already downloaded the Plain Text UTF-8 and **processed** it so you don’t have to.\n",
        "\n",
        "To get the processed text file, just follow these simple steps:\n",
        "\n",
        "1. go to the GitHub URL for this book: https://github.com/paperd/tensorflow\n",
        "2. locate the file: click **chapter8**, click **data**, click **two_cities.txt**\n",
        "3. click the **Raw** button\n",
        "4. copy the text (**Ctrl** + **a** + **c**)\n",
        "5. paste it into **Notepad** or another basic text editor (**Ctrl** + **v**)\n",
        "6. save it on your computer as **two_cities.txt**\n",
        "7. drag and drop file to your Google Drive **Colab Notebooks** folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wYl0uth1Y6HE"
      },
      "source": [
        "## Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYGWyMqnSkHu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a0b07fc2-4595-4fde-ee60-c1e53b466739"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhqHZIW7V535",
        "colab_type": "text"
      },
      "source": [
        "Click on the URL, choose a Google account, click **Allow**, copy the authorization code and paste it into Colab, and press the **Enter** key on your keyboard.\n",
        "\n",
        "Be sure that you have the file in the **Colab Notebooks** directory on your Google Drive!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gycVy8OxbMH2",
        "colab_type": "text"
      },
      "source": [
        "## Read the Corpus into Memory\n",
        "\n",
        "In NLP, a text document is often referred to as a corpus. A **corpus** is a collection of written texts, especially the entire works of a particular author or a body of writing on a particular subject."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eovE7E6b5J_",
        "colab_type": "text"
      },
      "source": [
        "Read data into memory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGEqNVljSkVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "two_cities = 'drive/My Drive/Colab Notebooks/two_cities.txt'\n",
        "\n",
        "with open(two_cities) as f:\n",
        "  corpus = f.read()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myx3hmuNcPlr",
        "colab_type": "text"
      },
      "source": [
        "## Verify Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMtfe2kK5DlU",
        "colab_type": "text"
      },
      "source": [
        "Display some text from the beginning of the corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hTKdUJsOX3Cl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "9fb5c866-e3f1-4dc4-e384-c26786ee55b1"
      },
      "source": [
        "print (corpus[:74])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A TALE OF TWO CITIES\n",
            "\n",
            "A STORY OF THE FRENCH REVOLUTION\n",
            "\n",
            "By Charles Dickens\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nFsmQ2gkL_bW"
      },
      "source": [
        "Since we start from the beginning, it is pretty easy to verify. But, verifying the end takes a bit more work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-vTlZZu5MYa",
        "colab_type": "text"
      },
      "source": [
        "Get the length of the corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48HyBcou5OiR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d9f0c39a-0456-48d3-f54e-8d3180d399ba"
      },
      "source": [
        "len(corpus)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "757247"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C-BsR-L5S7F",
        "colab_type": "text"
      },
      "source": [
        "Now, we know where the corpus ends."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFX4LXiZ5TsS",
        "colab_type": "text"
      },
      "source": [
        "With a bit of trial and error, we can display the famous quote from the end:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEPUEYbx5UXS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8913743b-a29b-4fdd-aa57-746731f339a5"
      },
      "source": [
        "print (corpus[757116:])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "“It is a far, far better thing that I do, than I have ever done; it is a\n",
            "far, far better rest that I go to than I have ever known.”\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf_Sjhka5aIj",
        "colab_type": "text"
      },
      "source": [
        "If you want to explore other online books for NLP, a great place to start is Project Gutenberg at the following URL: https://www.gutenberg.org/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w4bD7B8_9y5T"
      },
      "source": [
        "## Create Vocabulary\n",
        "\n",
        "Since our goal is to generate text with a character-level model, we train the model to predict the next character in a sequence. We can then repeatedly call the model to generate longer sequences of text. We begin creating a vocabulary of unique characters contained in the corpus and store it in **vocab**.    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQl5Jk_m5qvG",
        "colab_type": "text"
      },
      "source": [
        "Create a vocabulary of unique characters contained in the corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pd_iAwaH9t_9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3be35e06-73ab-4df9-8195-33b9c7170f0b"
      },
      "source": [
        "# unique characters in the corpus\n",
        "\n",
        "vocab = sorted(set(corpus))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "74 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g2HR7xnnAp38"
      },
      "source": [
        "So, we have a vocabulary of 74 unique characters in our corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FrgQx0575IcQ"
      },
      "source": [
        "## Vectorize the Text\n",
        "\n",
        "Algorithms process numbers, not text. So, we must devise a numerical representation of the corpus. An easy solution is to vectorize text. **Text vectorization** is the process of converting text into numerical representation.\n",
        "\n",
        "Let's start by creating dictionary **int_map** to hold integer mappings of unique characters. Next, create numpy array **char_map** to hold character mappings of each integer representation. The numpy array allows us to translate encoded integer mappings back into their character representations. Once we vectorize the corpus, we can build the input pipeline for TensorFlow consumption."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wQ5BNJqICJRN"
      },
      "source": [
        "## Create Integer Mappings \n",
        "\n",
        "We use dictionary comprehension to create **int_map**, which holds integer mappings for the corpus. **Dictionary comprehension** is a method for creating dictionaries using simple expressions. A dictionary comprehension takes the form **{key: value for (key, value) in iterable}**. In our case, the key is a unique character in the corpus and the value is the integer mapping of the unique character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3AaXYnGzDsQf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ac7be45b-a7bb-40bb-9c17-2a4857b651f0"
      },
      "source": [
        "# create a dictionary with integer representations of characters\n",
        "\n",
        "int_map = {key : value for value, key in enumerate(vocab)}\n",
        "int_map['a']"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E-hLIALFFhva"
      },
      "source": [
        "So, integer '46' represents the letter 'a'. Let's validate that this is the case:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p2XVypFd5Iuw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0bbe0ae9-d601-4522-cb8d-da32b447a57b"
      },
      "source": [
        "# create numpy array to hold character mappings of integers\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "char_map = np.array(vocab)\n",
        "char_map[45]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'a'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cDRN0c0yGkjb"
      },
      "source": [
        "It looks like our mappings work. Let's try it on a sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7AT9VGQgCNOG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "86f3cdbb-b52b-442b-b34d-c23fb43bb72a"
      },
      "source": [
        "# create variable to hold line break\n",
        "br = '\\n'\n",
        "\n",
        "# simple sequence\n",
        "sequence = 'hello world'\n",
        "print ('original sequence:', sequence, br)\n",
        "\n",
        "# map to integer representations\n",
        "maps = np.array([int_map[c] for c in sequence])\n",
        "print ('integer mappings:', maps, br)\n",
        "\n",
        "# map integer representations back into characters\n",
        "s = [char_map[i] for i in maps]\n",
        "\n",
        "# create string from list of characters\n",
        "s = ''.join(s)\n",
        "print ('translation:', s)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original sequence: hello world \n",
            "\n",
            "integer mappings: [52 49 56 56 59  1 67 59 62 56 48] \n",
            "\n",
            "translation: hello world\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iIzbf0AjB_Km"
      },
      "source": [
        "## Vectorize the Corpus\n",
        "\n",
        "Now, we are ready to vectorize the corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FjFyt3TeLcGA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "de0aee80-9c18-4a09-e0cb-faca8440b818"
      },
      "source": [
        "# vectorize the corpus\n",
        "encoded = np.array([int_map[c] for c in corpus])\n",
        "encoded[:20], char_map[encoded[:20]]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([19,  1, 38, 19, 30, 23,  1, 33, 24,  1, 38, 41, 33,  1, 21, 27, 38,\n",
              "        27, 23, 37]),\n",
              " array(['A', ' ', 'T', 'A', 'L', 'E', ' ', 'O', 'F', ' ', 'T', 'W', 'O',\n",
              "        ' ', 'C', 'I', 'T', 'I', 'E', 'S'], dtype='<U1'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OPUOIZgZ7p6",
        "colab_type": "text"
      },
      "source": [
        "Numpy array **encoded** holds the vectorized corpus. We display twenty integer mappings and their decodings to verify."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vEbDpFBIJgdU"
      },
      "source": [
        "## Predict the Next Character\n",
        "\n",
        "At each time step during training, our goal is to predict the next probable character given a character or a sequence of characters. So, input to the model must be a sequence of characters. To reach our goal, we must feed the model proper training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-DnrAZWQKy3y"
      },
      "source": [
        "## Create Training Input Sequences\n",
        "\n",
        "To create training intances, we divide the corpus into input sequences. Each input sequence contains *seq_length* characters from the corpus. The **seq_length** is the maximum length sentence we want for a single input sequence in characters. We break the corpus into equal length sequences for better performance.\n",
        "\n",
        "For each input sequence, the sample contains the text and the corresponding target contains the text shifted one character to the right. So, we break the text into chunks of *seq_length + 1*.\n",
        "\n",
        "Let's begin by converting the encoded corpus to tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P_QyVmfpC7jf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "08c5d33d-6072-4d90-b261-06c229dedc0a"
      },
      "source": [
        "# intialize maximum length sequence for a single input in characters\n",
        "\n",
        "seq_length = 100\n",
        "\n",
        "# create training dataset\n",
        "\n",
        "ds = tf.data.Dataset.from_tensor_slices(encoded)\n",
        "ds"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TensorSliceDataset shapes: (), types: tf.int64>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5E3gGioYnv8C"
      },
      "source": [
        "Display some samples from the TensorFlow dataset. We convert the integer representations of each character back to character state with the *char_map* array created earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7dtCQPbbTEgA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "22e21e71-73a1-4091-e163-e151370229a5"
      },
      "source": [
        "for i in ds.take(6):\n",
        "  print (i.numpy(), ':', char_map[i])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19 : A\n",
            "1 :  \n",
            "38 : T\n",
            "19 : A\n",
            "30 : L\n",
            "23 : E\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z-eQasP9VRv-"
      },
      "source": [
        "# Batch Sequences\n",
        "\n",
        "The batch method lets us easily convert individual characters to sequences of the desired size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nAiGP8ffDS1F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "511f7150-b724-4d1b-c79d-24b2b717cb75"
      },
      "source": [
        "sequences = ds.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "for i in sequences.take(1):\n",
        "  print (char_map[i], br)\n",
        "  print ('batch size:', len(i))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['A' ' ' 'T' 'A' 'L' 'E' ' ' 'O' 'F' ' ' 'T' 'W' 'O' ' ' 'C' 'I' 'T' 'I'\n",
            " 'E' 'S' '\\n' '\\n' 'A' ' ' 'S' 'T' 'O' 'R' 'Y' ' ' 'O' 'F' ' ' 'T' 'H' 'E'\n",
            " ' ' 'F' 'R' 'E' 'N' 'C' 'H' ' ' 'R' 'E' 'V' 'O' 'L' 'U' 'T' 'I' 'O' 'N'\n",
            " '\\n' '\\n' 'B' 'y' ' ' 'C' 'h' 'a' 'r' 'l' 'e' 's' ' ' 'D' 'i' 'c' 'k' 'e'\n",
            " 'n' 's' '\\n' '\\n' '\\n' 'C' 'O' 'N' 'T' 'E' 'N' 'T' 'S' '\\n' '\\n' '\\n' ' '\n",
            " ' ' ' ' ' ' ' ' 'B' 'o' 'o' 'k' ' ' 't' 'h' 'e'] \n",
            "\n",
            "batch size: 101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cxec3Z0TfZLp",
        "colab_type": "text"
      },
      "source": [
        "Batch size is 101 to account for the shift of 1 character to the right for the target."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z4lHWmjqWHGH"
      },
      "source": [
        "# Create Samples and Targets\n",
        "\n",
        "For each input sequence, use the **map** method to apply the *create_sample_target* function. The function shifts an input sequence by *1* to form the sample and target texts for each batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yIJYdHNQWHPV",
        "colab": {}
      },
      "source": [
        "def create_sample_target(piece):\n",
        "  sample = piece[:-1]\n",
        "  target = piece[1:]\n",
        "  return sample, target\n",
        "\n",
        "dataset = sequences.map(create_sample_target)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8xVgM-hzYdxg"
      },
      "source": [
        "Display the first split input sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kfznKHH8Yd7w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "eb869a90-7d90-471b-e689-62c77b46c9d0"
      },
      "source": [
        "for sample, target in  dataset.take(1):\n",
        "  print ('sample:', char_map[sample], br)\n",
        "  print ('target:', char_map[target])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample: ['A' ' ' 'T' 'A' 'L' 'E' ' ' 'O' 'F' ' ' 'T' 'W' 'O' ' ' 'C' 'I' 'T' 'I'\n",
            " 'E' 'S' '\\n' '\\n' 'A' ' ' 'S' 'T' 'O' 'R' 'Y' ' ' 'O' 'F' ' ' 'T' 'H' 'E'\n",
            " ' ' 'F' 'R' 'E' 'N' 'C' 'H' ' ' 'R' 'E' 'V' 'O' 'L' 'U' 'T' 'I' 'O' 'N'\n",
            " '\\n' '\\n' 'B' 'y' ' ' 'C' 'h' 'a' 'r' 'l' 'e' 's' ' ' 'D' 'i' 'c' 'k' 'e'\n",
            " 'n' 's' '\\n' '\\n' '\\n' 'C' 'O' 'N' 'T' 'E' 'N' 'T' 'S' '\\n' '\\n' '\\n' ' '\n",
            " ' ' ' ' ' ' ' ' 'B' 'o' 'o' 'k' ' ' 't' 'h'] \n",
            "\n",
            "target: [' ' 'T' 'A' 'L' 'E' ' ' 'O' 'F' ' ' 'T' 'W' 'O' ' ' 'C' 'I' 'T' 'I' 'E'\n",
            " 'S' '\\n' '\\n' 'A' ' ' 'S' 'T' 'O' 'R' 'Y' ' ' 'O' 'F' ' ' 'T' 'H' 'E' ' '\n",
            " 'F' 'R' 'E' 'N' 'C' 'H' ' ' 'R' 'E' 'V' 'O' 'L' 'U' 'T' 'I' 'O' 'N' '\\n'\n",
            " '\\n' 'B' 'y' ' ' 'C' 'h' 'a' 'r' 'l' 'e' 's' ' ' 'D' 'i' 'c' 'k' 'e' 'n'\n",
            " 's' '\\n' '\\n' '\\n' 'C' 'O' 'N' 'T' 'E' 'N' 'T' 'S' '\\n' '\\n' '\\n' ' ' ' '\n",
            " ' ' ' ' ' ' 'B' 'o' 'o' 'k' ' ' 't' 'h' 'e']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J6cWtgAP2wr9"
      },
      "source": [
        "Notice that the target is one character ahead of the sample. We do this so the algorithm can learn from the target how to predict the next character."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5gwG-zRWZShI"
      },
      "source": [
        "# Time Step Prediction\n",
        "\n",
        "Each index of the sample and target vectors is processed as a single time step. That is, each character processed in a sample and target is a time step. So, for the input at time step 0, the model receives the index (input_idx) for **A** and tries to predict the index for **a space** as the next character. At the next time step, the model repeats the same process. But, the RNN model considers the context of the previous step in addition to the current input character. The output verifies that the sample and target sets were created properly.\n",
        "\n",
        "Let't look at the first 5 timesteps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kgrEdFFbZSqf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "e2fbd8d5-2e86-40bb-cf67-2737c0ae4281"
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(\n",
        "    zip(sample[:5], target[:5])):\n",
        "  print('Step:', i)\n",
        "  print(' input:', input_idx.numpy(),\n",
        "        char_map[input_idx])\n",
        "  print(' expected output:', target_idx.numpy(),\n",
        "        char_map[target_idx])\n",
        "  if i < 4: print()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step: 0\n",
            " input: 19 A\n",
            " expected output: 1  \n",
            "\n",
            "Step: 1\n",
            " input: 1  \n",
            " expected output: 38 T\n",
            "\n",
            "Step: 2\n",
            " input: 38 T\n",
            " expected output: 19 A\n",
            "\n",
            "Step: 3\n",
            " input: 19 A\n",
            " expected output: 30 L\n",
            "\n",
            "Step: 4\n",
            " input: 30 L\n",
            " expected output: 23 E\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tf0eg1C3V7HU"
      },
      "source": [
        "As we can see, the sample and target data were created properly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r2zQ0xH9cjOx"
      },
      "source": [
        "# Create Training Batches\n",
        "\n",
        "We already split the text into manageable sequences. But, before feeding the data into the model we need to shuffle it and pack it into batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bti5JINtcjYD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7dad775a-6712-44a5-cc0a-c8522a8a7124"
      },
      "source": [
        "# batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# buffer size\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "corpus_ds = (dataset\n",
        "  .shuffle(BUFFER_SIZE)\n",
        "  .batch(BATCH_SIZE, drop_remainder=True)\n",
        "  .cache().prefetch(1))\n",
        "\n",
        "corpus_ds"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gmx87oI3dCKE"
      },
      "source": [
        "TensorFlow data is designed to work with infinite sequences. So, it doesn't attempt to shuffle the entire sequence in memory. Instead, it maintains a buffer where it shuffles elements. We set *BUFFER_SIZE=10000* to give TensorFlow a fairly large buffer size, but not too big that we cause memory issues. We can see that our dataset contains training samples and targets with batch sizes of 64 and sequence lengths of 100."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TTO-XRmZfdz8"
      },
      "source": [
        "# Build the Model\n",
        "\n",
        "Begin by initializing some important variables. We set *vocab_size* to the number of unique characters in the corpus. We set *embedding dimension* to 256. **Word embedding** is a learning technique in NLP where words or phrases from the vocabulary are mapped to vectors of real numbers. In practice, we use word embedding vectors with dimensions between 50 and 500. We use *256*, which we believe is a nice compromise between processing time and performance. The higher the number of word embeddings, the more performance we can squeeze out of our model. But, higher embedding dimensions are computationally expensive. We set *rnn_units* to 1024, which represents the number of neurons output from a layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M_DvCzfKd5JW",
        "colab": {}
      },
      "source": [
        "# length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# the embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3oh7DoXAHV_J"
      },
      "source": [
        "## Generate Seed and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qzQk21-DHnAk",
        "colab": {}
      },
      "source": [
        "# plant random seeds for reproducibility\n",
        "tf.random.set_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "# clear any previous models\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# import libraries\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense,\\\n",
        "Embedding\n",
        "from tensorflow.keras import losses"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p7Umiu46H1GH"
      },
      "source": [
        "## Create Layers\n",
        "\n",
        "The first layer is an embedding layer with the vocabulary size, embedding dimensions, and the input shape of the batch as inputs. The output from the embedding layer feeds into the second layer, which is a GRU layer with 1024 neurons (identified by the *rnn_units* variable). To retain what was learned at this layer, we set *return_sequences=True* and *stateful=True*. We also want to tell the GRU layer to draw samples from a uniform distribution so we set *recurrent_initializer='glorot_uniform'*. The output from the GRU layer feeds into the final Dense layer with vocabulary size as input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mG8FNlMhH0fv",
        "colab": {}
      },
      "source": [
        "# create the model\n",
        "\n",
        "model = Sequential([\n",
        "  Embedding(vocab_size, embedding_dim,\n",
        "            batch_input_shape=[BATCH_SIZE, None]),\n",
        "  GRU(rnn_units, return_sequences=True,\n",
        "      stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "  Dense(vocab_size)\n",
        "])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_ALtMqMCIZPX"
      },
      "source": [
        "## Display Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KPIuc_4jIZaY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "84905aa7-6ba6-4864-d6ac-e1ab8592a1c7"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           18944     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 74)            75850     \n",
            "=================================================================\n",
            "Total params: 4,033,098\n",
            "Trainable params: 4,033,098\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWpf1MUim7jm",
        "colab_type": "text"
      },
      "source": [
        "The first layer is an embedding. So, calculate the number of learnable parameters by multiplying vobulary size of 74 by embedding dimension of 256 for a total of 18,944. \n",
        "\n",
        "The second layer is a GRU. The number of learnable parameters is thereby based on the formula **3 x (n<sup>2</sup> x mn + 2n)** where *m* is the input dimension and *n* is the output dimension. Multiply by *3* because there are three sets of operations requiring weight matrices of these sizes. Multiply n by *2* because of the feedback loops of a RNN. So, we get 3,938,304 learnable parameters. Here's how we break down the result:\n",
        "* 3 x (1024<sup>2</sup> + 1024 x 256 + 2 x 1024)\n",
        "* 3 x (1048576 + 262144 + 2048)\n",
        "* 3 x 1312768\n",
        "* 3,938,304\n",
        "\n",
        "Calculating learnable parameters for the second layer is pretty complex. Let's break it down logically. A GRU layer is a feedfoward layer with feedback loops. Learnable parameters for a feedforward network are calculated by multiplying output from the previous layer (256 neurons) with neurons at the current layer (1024 neurons). With a feedforward network, we also have to account for the 1024 neurons at this layer. But, we multiply the 1024 neurons at this layer by 2 because of the feedback mechanism of a RNN. Finally, the current layer's 1024 neurons are fed back resulting in 1024<sup>2</sup> learnable parameters. A GRU uses three sets of operations (hidden state, reset gate, and update gate) requiring weight matrices, so we multiply the learnable parameters by 3.\n",
        "\n",
        "The third layer is dense. So, calculate the number of learnable parameters by multiplying output dimension of 74 by input dimension of 1,024 and adding 74 to account for the number of neurons at this layer for a total of 75,850."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3jpRSlq_snN",
        "colab_type": "text"
      },
      "source": [
        "For a deep discussion of GRUs, consult the following URL: https://arxiv.org/ftp/arxiv/papers/1701/1701.05923.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HxUKfCsvI4VK"
      },
      "source": [
        "## Check Output Shape\n",
        "\n",
        "Display the shape of the first batch in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8JaFEmIUd6RO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "207f3f34-5f90-4934-8375-e62372f036a5"
      },
      "source": [
        "for sample, target in corpus_ds.take(1):\n",
        "  example_batch_predictions = model(sample)\n",
        "  \n",
        "example_batch_predictions.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 100, 74])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lOISokd0JYs2"
      },
      "source": [
        "So, the first batch has *batch_size* of 64, *sequence_length* of 100, and *vocab_size* of 74 as expected. Notice that the output shape from displaying model.summary() is (64, None, 74). The sequence length is not included because the model can be run on inputs of any length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "olv8Y_ncT3g7"
      },
      "source": [
        "## Calculate Loss\n",
        "\n",
        "We sample from the output distribution to predict character indices. The output distribution is defined by the logits over our character vocabulary. A **logit** is a probability value between 0 and 1, and negative infinity to infinity derived from a logit function. Simply, a logit is a prediction. The logit function is an inverse to the sigmoid function as it limits values between 0 and 1 across the Y-axis rather than the X-axis. Since our model returns logits, we need to set the **from_logits** flag to calculate loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nuHnIw7LUHP7",
        "colab": {}
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return losses.sparse_categorical_crossentropy(\n",
        "      labels, logits, from_logits=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eEUe_f3bjFkY"
      },
      "source": [
        "The model expects a 3D tensor consisting of batch size, sequence length, and vocabulary size. So, let's test our loss function that it is working as expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bnRPA1ZvjNOb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8cb56aa8-dd09-4c7e-a9fe-8fecba67f3f5"
      },
      "source": [
        "pre_trained_loss = loss(target, example_batch_predictions)\n",
        "\n",
        "print('pred shape: ', example_batch_predictions.shape)\n",
        "print('scalar_loss: ', pre_trained_loss.numpy().mean())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pred shape:  (64, 100, 74)\n",
            "scalar_loss:  4.302785\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oddRB8HAXf6_"
      },
      "source": [
        "Great! We can see that the prediction shape has batch size of 64, sequence length of 100, and vocabulary size of 74. We also display the average loss from the pretrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2ThTMfeAMALm"
      },
      "source": [
        "## Compile the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "45zULFR9JqY9",
        "colab": {}
      },
      "source": [
        "model.compile(loss=loss,\n",
        "              optimizer='adam')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VxkjDFVXQCMf"
      },
      "source": [
        "## Configure Checkpoints\n",
        "\n",
        "With a RNN, we want to save what the model learned at each timestep. One way to do this is to save the checkpoints that hold this information with a callback method. **Checkpoints** capture the exact value of all TensorFlow parameters used by a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VfyuuU6WP_13",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "# directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "\n",
        "# name of the checkpoint files\n",
        "checkpoint_files = os.path.join(checkpoint_dir,\n",
        "                                'ckpt_{epoch}')\n",
        "\n",
        "# callback method\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_files,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eSTOYZKVLB3E"
      },
      "source": [
        "## Train the Model\n",
        "\n",
        "Let's train the model on 10 epochs. You can add epochs to improve performance. We tell the model to save checkpoints with **checkpoint_callback**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qhml623-K_vy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "d9f8d6b8-8fd0-4522-d6dd-5f742a30b062"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "history = model.fit(corpus_ds, epochs=EPOCHS,\n",
        "                    callbacks=[checkpoint_callback])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "117/117 [==============================] - 5s 44ms/step - loss: 2.8533\n",
            "Epoch 2/10\n",
            "117/117 [==============================] - 5s 43ms/step - loss: 2.1272\n",
            "Epoch 3/10\n",
            "117/117 [==============================] - 5s 44ms/step - loss: 1.8593\n",
            "Epoch 4/10\n",
            "117/117 [==============================] - 5s 43ms/step - loss: 1.6703\n",
            "Epoch 5/10\n",
            "117/117 [==============================] - 5s 43ms/step - loss: 1.5385\n",
            "Epoch 6/10\n",
            "117/117 [==============================] - 5s 43ms/step - loss: 1.4452\n",
            "Epoch 7/10\n",
            "117/117 [==============================] - 5s 43ms/step - loss: 1.3746\n",
            "Epoch 8/10\n",
            "117/117 [==============================] - 5s 43ms/step - loss: 1.3175\n",
            "Epoch 9/10\n",
            "117/117 [==============================] - 5s 43ms/step - loss: 1.2677\n",
            "Epoch 10/10\n",
            "117/117 [==============================] - 5s 43ms/step - loss: 1.2222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0w554-QKNCCm"
      },
      "source": [
        "## Rebuild Model for Text Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UKEIyMl5NCjG"
      },
      "source": [
        "### Restore Weights from Checkpoints\n",
        "\n",
        "First, restore the weights from the checkpoints we established during training. We need to restore the checkpoints to obtain what the RNN learned at each time step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Lkm1AyznRTWu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "30ee6aa1-1e4d-4e1b-f964-1698e047a436"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./training_checkpoints/ckpt_10'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qr19ouNYbQGS"
      },
      "source": [
        "### Rebuild with Batch Size of 1\n",
        "\n",
        "Second, to keep prediction simple we use a batch size of 1. Because of the way the RNN state is passed from timestep to timestep, the model only accepts a fixed batch size once built. So, let's rebuild it with batch size of 1 (instead of 64)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YDFwokBYM46n",
        "colab": {}
      },
      "source": [
        "# generate seed for reproducibility\n",
        "tf.random.set_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "# clear any previous models\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# set batch size to 1\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "# Rebuild model\n",
        "model = Sequential([\n",
        "  Embedding(vocab_size, embedding_dim,\n",
        "            batch_input_shape=[BATCH_SIZE, None]),\n",
        "  GRU(rnn_units, return_sequences=True,\n",
        "      stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "  Dense(vocab_size)\n",
        "])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zrfLet5ibW3Y"
      },
      "source": [
        "### Load Weights and Reshape\n",
        "\n",
        "Third, load the weights and reshape the model to ensure that tensors have batch size of 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xcrKAmJhRjvA",
        "colab": {}
      },
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8kSKyElNRr8n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "7550e637-cfd5-4e14-d3aa-23ed373259b4"
      },
      "source": [
        "# good idea to view model at this point\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (1, None, 256)            18944     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (1, None, 74)             75850     \n",
            "=================================================================\n",
            "Total params: 4,033,098\n",
            "Trainable params: 4,033,098\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Sahcc-BKN9cF"
      },
      "source": [
        "All is well!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8UF6rwabbjUQ"
      },
      "source": [
        "## Create New Text\n",
        "\n",
        "To create new text, create a function and initialize a set of variables to feed to the function. To prepare the starting string for TensorFlow consumption, vectorize and reshape it before passing it to the function. Let's begin with the function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iqCgvXf2use5"
      },
      "source": [
        "### Create the Function\n",
        "\n",
        "The function accepts the model, vectorized starting string, temperature, and the original starting string. It begins by intializing a list to hold the new text created and resetting the states of the model. The function continues by iterating **n** times (the number of characters we wish to create).\n",
        "\n",
        "**Temperature** is a hyperparameter of neural networks used to control the randomness of predictions by scaling the logits before applying softmax. A bit more simply, temperature represents how much to divide the logits by before computing softmax.\n",
        "\n",
        "During iteration, the function models the encoded starting string and places the result in **predictions**. It then removes the extra '1' dimension so it can divide the contents of 'predictions' by the temperature. The next task of the function is to use a categorical distribution to predict the next character returned by the model. The function needs to add the '1' dimension back so that it can pass the predicted character as the next input to the model along with the previous hidden state. The process repeats until the loop is extinghuised."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LNdsqkRvoG2D",
        "colab": {}
      },
      "source": [
        "def create_text(model, input_eval, temperature, start_string):\n",
        "\n",
        "  # Empty string to store our results\n",
        "  new_text = []\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "\n",
        "  for i in range(n):\n",
        "    # model encoded input\n",
        "    predictions = model(input_eval)\n",
        "\n",
        "    # remove batch dimension so we can manipulate predictions\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # divide predictions by temperature\n",
        "    predictions = predictions / temperature\n",
        "\n",
        "    # use a categorical distribution to predict character\n",
        "    # returned by model\n",
        "    predicted_id = tf.random.categorical(\n",
        "        predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # pass predicted character as next input to model\n",
        "    # with previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    # append generated characters to text\n",
        "    new_text.append(char_map[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(new_text))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qmPCepgvPHkn"
      },
      "source": [
        "### Initialize Variables\n",
        "\n",
        "Now that we have the function, let's initialize. We begin by setting n to the number of characters we wish to create. We continue by setting the temperature and the starting string. Low temperatures result in more predictable text, while higher temperatures result in more surprising text. You can experiment to find the best setting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZZvxbhyponH2",
        "colab": {}
      },
      "source": [
        "n = 500\n",
        "temp = 0.3\n",
        "start_string = 'Tale'"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PjWUY5CYkkPR"
      },
      "source": [
        "We can try different start strings, but we chose 'Tale' because we know that the corpus contains this name."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aZIr1hkeSiCb"
      },
      "source": [
        "### Vectorize and Reshape Starting String\n",
        "\n",
        "We need to vectorize the starting string because the model only recognizes numbers. We need to reshape the vectorized starting string for TensorFlow consumption. We display the shapes to verify that all is well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MGsgR3cSShah",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "c487fc33-d24c-44e6-9301-fc6f6e379a00"
      },
      "source": [
        "# vectorize starting string\n",
        "input_vectorized = [int_map[s] for s in start_string]\n",
        "print ('original shape:', end=' ')\n",
        "print (str(np.array(input_vectorized).ndim) + 'D', br)\n",
        "\n",
        "# reshape string for TensorFlow model consumption\n",
        "input_vectorized = tf.expand_dims(input_vectorized, 0)\n",
        "print ('new shape:', input_vectorized.shape)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original shape: 1D \n",
            "\n",
            "new shape: (1, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kWEIqdleSeTC"
      },
      "source": [
        "### Create New Text with Function Invocation\n",
        "\n",
        "We are now ready to create new text. Plant random seeds for reproducibility. Next, call the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dzeO_iql0Hgp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b89cac41-9cb6-4040-8097-4a77cb595ccc"
      },
      "source": [
        "tf.random.set_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "print (create_text(model, input_vectorized, temp, start_string))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tale the ways of the chair that was not my patriot with a daughter, and that they were near the manner of the paper than he came to the chair that the traveller was the way of the paper than the days when the chateau was closed to himself. “He had been the case while I can't say that the paper than the chair to the case which he came to the case of the name of the way of the chair was the way of the case when they were left them that the way to the chateau with the carriage of the chair than the cha\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u3OB9JMiWf7Q"
      },
      "source": [
        "Wow! Although the sentences are nonsensical, the model creates actual sentences."
      ]
    }
  ]
}